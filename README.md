Fluxo de Pipeline usando GCP (Dataflow & BigQuery)
Este projeto demonstra como criar e executar um pipeline de dados utilizando o Google Cloud Platform (GCP), com foco no processamento de arquivos CSV armazenados no Cloud Storage, tratamento dos dados com Apache Beam no Dataflow, e carregamento das tabelas tratadas no BigQuery. AlÃ©m disso, o resultado final Ã© integrado ao Power BI para visualizaÃ§Ã£o de dados.
ğŸ“‚ Estrutura do Projeto

nexus/
â”œâ”€â”€ arquivos/          # Pasta para arquivos CSV locais
â”œâ”€â”€ config/            # ConfiguraÃ§Ãµes, incluindo chave de serviÃ§o
â”œâ”€â”€ log/               # Arquivos de log gerados
â”œâ”€â”€ service/           # FunÃ§Ãµes auxiliares (e.g., upload para GCS)
â”œâ”€â”€ venv/              # Ambiente virtual (nÃ£o incluÃ­do no Git)
â”œâ”€â”€ pipeline.py        # CÃ³digo principal do pipeline (Apache Beam)
â”œâ”€â”€ app.py             # Script para simular o envio de arquivos para GCS
â”œâ”€â”€ requirements.txt   # DependÃªncias do projeto

ğŸš€ Funcionalidades
1. **SimulaÃ§Ã£o de Produtor**:
   - Um script em Python (`app.py`) simula o envio de arquivos CSV para o Cloud Storage.
2. **Pipeline de Processamento**:
   - Processa os dados utilizando Apache Beam e o Dataflow.   - Verifica, limpa e transforma os arquivos CSV em trÃªs tabelas diferentes no BigQuery.
3. **ConexÃ£o ao Power BI**:
   - Os dados processados no BigQuery sÃ£o conectados ao Power BI para criar relatÃ³rios visuais.
ğŸ› ï¸ Passos de ConfiguraÃ§Ã£o
### 1. **PrÃ©-requisitos**
- **Crie os recursos no GCP**:
  - Projeto no GCP
  - Bucket no Cloud Storage (com subpastas: `templates`, `temp`, `csv_files`)
  - Dataset no BigQuery
- **Configure a mesma regiÃ£o para todos os recursos** (e.g., `us-west1`).
### 2. **Habilite APIs NecessÃ¡rias**
- Ative as seguintes APIs no GCP:
  - Cloud Storage
  - Dataflow
  - BigQuery
  - BigQuery Storage
### 3. **Crie uma Conta de ServiÃ§o**
- Gere uma chave de serviÃ§o em formato `.json` e salve na pasta `config/`.
### 4. **Instale DependÃªncias**
- Crie e ative um ambiente virtual:
  ```bash
  python -m venv venv
  source venv/bin/activate # ou venv/Scripts/activate no Windows
  ```
- Instale as dependÃªncias:
  ```bash
  pip install -r requirements.txt
  ```
ğŸ“„ DescriÃ§Ã£o do Pipeline
1. **Leitura de Arquivos CSV**:
- Os arquivos no GCS sÃ£o lidos conforme padrÃµes definidos, como:
  - `gs://csv_files/customer*.csv`
  - `gs://csv_files/transactions_file1*.csv`
  - `gs://csv_files/transactions_file2*.csv`
2. **TransformaÃ§Ã£o e ValidaÃ§Ã£o**:
- Classes de parsing verificam:
  - Datas vÃ¡lidas
  - Valores nÃ£o nulos
  - Tipos de dados corretos
- Campos sÃ£o convertidos e padronizados.
3. **Escrita no BigQuery**:
- Os dados tratados sÃ£o salvos em tabelas BigQuery:
  - `customers_file3`
  - `transactions_file1`
  - `transactions_file2`
4. **ExecuÃ§Ã£o no Dataflow**:
- O pipeline Ã© executado localmente para testes e no Dataflow para produÃ§Ã£o.
ğŸ—‚ï¸ Esquemas das Tabelas (BigQuery)
customers_file3

CREATE TABLE `.customers_file3` (
  customer_id STRING,
  customer_name STRING,
  customer_email STRING
);

transactions_file1

CREATE TABLE `.transactions_file1` (
  transaction_id STRING,
  customer_id STRING,
  transaction_date DATE,
  transaction_amount FLOAT64,
  transaction_status STRING
);

transactions_file2

CREATE TABLE `.transactions_file2` (
  transaction_id STRING,
  transaction_type STRING,
  qtty FLOAT64,
  price FLOAT64
);

ğŸ“Š IntegraÃ§Ã£o com Power BI
ApÃ³s o processamento dos dados no BigQuery, conecte o Power BI:
- Escolha o mÃ©todo **Importar Dados** para maior desempenho.
- Crie relatÃ³rios visuais com base nas tabelas do BigQuery.
ğŸ§‘â€ğŸ’» Autor
- **Vinicius Farineli Freire**
Criado em **08/12/2024**
