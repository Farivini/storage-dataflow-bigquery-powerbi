Fluxo de Pipeline usando GCP (Dataflow & BigQuery)

üìÇ Estrutura do Projeto

nexus/
arquivos/          # Pasta para arquivos CSV locais
config/            # Configura√ß√µes, incluindo chave de servi√ßo
log/               # Arquivos de log gerados
service/           # Fun√ß√µes auxiliares (e.g., upload para GCS)
venv/              # Ambiente virtual (n√£o inclu√≠do no Git)
pipeline.py        # C√≥digo principal do pipeline (Apache Beam)
app.py             # Script para simular o envio de arquivos para GCS
requirements.txt   # Depend√™ncias do projeto

üöÄ Funcionalidades
1. **Simula√ß√£o de Produtor**:
   - Um script em Python (`app.py`) simula o envio de arquivos CSV para o Cloud Storage.
2. **Pipeline de Processamento**:
   - Processa os dados utilizando Apache Beam e o Dataflow.   - Verifica, limpa e transforma os arquivos CSV em tr√™s tabelas diferentes no BigQuery.
3. **Conex√£o ao Power BI**:
   - Os dados processados no BigQuery s√£o conectados ao Power BI para criar relat√≥rios visuais.
üõ†Ô∏è Passos de Configura√ß√£o
### 1. **Pr√©-requisitos**
- **Crie os recursos no GCP**:
  - Projeto no GCP
  - Bucket no Cloud Storage (com subpastas: `templates`, `temp`, `csv_files`)
  - Dataset no BigQuery
- **Configure a mesma regi√£o para todos os recursos** (e.g., `us-west1`).
### 2. **Habilite APIs Necess√°rias**
- Ative as seguintes APIs no GCP:
  - Cloud Storage
  - Dataflow
  - BigQuery
  - BigQuery Storage
### 3. **Crie uma Conta de Servi√ßo**
- Gere uma chave de servi√ßo em formato `.json` e salve na pasta `config/`.
### 4. **Instale Depend√™ncias**
- Crie e ative um ambiente virtual:
  ```bash
  python -m venv venv
  source venv/bin/activate # ou venv/Scripts/activate no Windows
  ```
- Instale as depend√™ncias:
  ```bash
  pip install -r requirements.txt
  ```
üìÑ Descri√ß√£o do Pipeline
1. **Leitura de Arquivos CSV**:
- Os arquivos no GCS s√£o lidos conforme padr√µes definidos, como:
  - `gs://csv_files/customer*.csv`
  - `gs://csv_files/transactions_file1*.csv`
  - `gs://csv_files/transactions_file2*.csv`
2. **Transforma√ß√£o e Valida√ß√£o**:
- Classes de parsing verificam:
  - Datas v√°lidas
  - Valores n√£o nulos
  - Tipos de dados corretos
- Campos s√£o convertidos e padronizados.
3. **Escrita no BigQuery**:
- Os dados tratados s√£o salvos em tabelas BigQuery:
  - `customers_file3`
  - `transactions_file1`
  - `transactions_file2`
4. **Execu√ß√£o no Dataflow**:
- O pipeline √© executado localmente para testes e no Dataflow para produ√ß√£o.

  
üóÇÔ∏è Esquemas das Tabelas (BigQuery)
customers_file3

CREATE TABLE `.customers_file3` (
  customer_id STRING,
  customer_name STRING,
  customer_email STRING
);

transactions_file1

CREATE TABLE `.transactions_file1` (
  transaction_id STRING,
  customer_id STRING,
  transaction_date DATE,
  transaction_amount FLOAT64,
  transaction_status STRING
);

transactions_file2

CREATE TABLE `.transactions_file2` (
  transaction_id STRING,
  transaction_type STRING,
  qtty FLOAT64,
  price FLOAT64
);

üìä Integra√ß√£o com Power BI
Ap√≥s o processamento dos dados no BigQuery, conecte o Power BI:
- Escolha o m√©todo **Importar Dados** para maior desempenho.
- Crie relat√≥rios visuais com base nas tabelas do BigQuery.
üßë‚Äçüíª Autor
- **Vinicius Farineli Freire**
Criado em **08/12/2024**
